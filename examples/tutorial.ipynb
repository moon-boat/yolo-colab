{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO11 Tutorial",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy1nQDPgcPZv",
        "outputId": "c0919157-6165-4314-a5e0-cfc30f2861b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "project_path = '/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test'\n",
        "os.chdir(project_path)"
      ],
      "metadata": {
        "id": "6jmPdT0ldU6A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar -xzf /content/gdrive/MyDrive/TensorRT-10.10.0.31.Linux.x86_64-gnu.cuda-11.8.tar.gz -C /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "yGqk6OtMgbjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看Python版本\n",
        "!python --version\n",
        "\n",
        "# 安装TensorRT wheel文件（针对Python 3.12）\n",
        "!pip install /content/gdrive/MyDrive/TensorRT-10.10.0.31/python/tensorrt-10.10.0.31-cp312-none-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtXLg9ULhf4H",
        "outputId": "97e49ed7-45ca-44c0-8cf5-3d6d739ccc0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n",
            "Processing /content/gdrive/MyDrive/TensorRT-10.10.0.31/python/tensorrt-10.10.0.31-cp312-none-linux_x86_64.whl\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-10.10.0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 设置并持久化 LD_LIBRARY_PATH 环境变量\n",
        "# import os\n",
        "# os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib'\n",
        "\n",
        "# # 验证 LD_LIBRARY_PATH\n",
        "# print(os.environ['LD_LIBRARY_PATH'])\n",
        "!mkdir -p /content/lib\n",
        "!cp -r /content/gdrive/MyDrive/TensorRT-10.10.0.31/targets/x86_64-linux-gnu/lib/* /content/lib/\n",
        "# 更新 LD_LIBRARY_PATH\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/content/lib'\n",
        "print(os.environ['LD_LIBRARY_PATH'])\n",
        "# 更新缓存\n",
        "!sudo ldconfig /content/lib\n",
        "!ldconfig -p | grep libnvinfer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWkEekedmMOt",
        "outputId": "b303422b-3f65-44a5-f1fd-4c11f093652c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib64-nvidia:/content/lib\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "\tlibnvinfer_vc_plugin.so.10 (libc6,x86-64) => /content/lib/libnvinfer_vc_plugin.so.10\n",
            "\tlibnvinfer_vc_plugin.so (libc6,x86-64) => /content/lib/libnvinfer_vc_plugin.so\n",
            "\tlibnvinfer_plugin.so.10 (libc6,x86-64) => /content/lib/libnvinfer_plugin.so.10\n",
            "\tlibnvinfer_plugin.so (libc6,x86-64) => /content/lib/libnvinfer_plugin.so\n",
            "\tlibnvinfer_lean.so.10 (libc6,x86-64) => /content/lib/libnvinfer_lean.so.10\n",
            "\tlibnvinfer_lean.so (libc6,x86-64) => /content/lib/libnvinfer_lean.so\n",
            "\tlibnvinfer_dispatch.so.10 (libc6,x86-64) => /content/lib/libnvinfer_dispatch.so.10\n",
            "\tlibnvinfer_dispatch.so (libc6,x86-64) => /content/lib/libnvinfer_dispatch.so\n",
            "\tlibnvinfer.so.10 (libc6,x86-64) => /content/lib/libnvinfer.so.10\n",
            "\tlibnvinfer.so (libc6,x86-64) => /content/lib/libnvinfer.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/sunsmarterjie/yolov12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWwb_w7ijV1Z",
        "outputId": "61abe94e-be9e-4c48-a0b0-0b007071794e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov12'...\n",
            "remote: Enumerating objects: 1159, done.\u001b[K\n",
            "remote: Counting objects: 100% (479/479), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 1159 (delta 324), reused 296 (delta 296), pack-reused 680 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1159/1159), 1.81 MiB | 8.89 MiB/s, done.\n",
            "Resolving deltas: 100% (580/580), done.\n",
            "Updating files: 100% (332/332), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov12/\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f4p1H60kTSL",
        "outputId": "4299d05c-e749-4360-aef1-4a1c05397b7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12\n",
            "Obtaining file:///content/gdrive/MyDrive/Colab%20Notebooks/TensorRT%20Colab%20Test/yolov12\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.63) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics==8.3.63)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.63) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.63) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.63) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.63) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.63) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.63) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.3.63) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.63) (3.0.2)\n",
            "Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Building wheels for collected packages: ultralytics\n",
            "  Building editable for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ultralytics: filename=ultralytics-8.3.63-0.editable-py3-none-any.whl size=20281 sha256=a80f406222afdb277245a637306c84aa11d6a17b65b58bdf21d41148e8aa574c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k1q3f8er/wheels/0e/b7/81/e54bb0174b7aa188cc0f4af9bc0ad331274d58870dac8de82b\n",
            "Successfully built ultralytics\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.63 ultralytics-thop-2.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be2dffa-acdd-4bef-c768-e1f2b1dcba19"
      },
      "source": [
        "import ultralytics\n",
        "import os\n",
        "os.chdir(project_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/yolov12/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !yolo export model=yolo11n.pt format=engine half=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "54cf4ec3-22ac-42f7-9be0-3274dc008aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.48 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.7s, saved as 'yolo11n.onnx' (10.2 MB)\n",
            "ERROR ❌ \u001b[34m\u001b[1mTensorRT:\u001b[0m export failure 1.7s: generic_type: type \"FallbackString\" is already registered!\n",
            "KeyboardInterrupt\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/exporter.py\", line 919, in export_engine\n",
            "    import tensorrt as trt  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt/__init__.py\", line 83, in <module>\n",
            "    from .tensorrt import *\n",
            "ImportError: initialization failed\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/cfg/__init__.py\", line 986, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/model.py\", line 734, in export\n",
            "    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/exporter.py\", line 488, in __call__\n",
            "    f[1], _ = self.export_engine(dla=dla)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/exporter.py\", line 199, in outer_func\n",
            "    raise e\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/exporter.py\", line 194, in outer_func\n",
            "    f, model = inner_func(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/ultralytics/ultralytics/engine/exporter.py\", line 923, in export_engine\n",
            "    import tensorrt as trt  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt/__init__.py\", line 83, in <module>\n",
            "    from .tensorrt import *\n",
            "ImportError: generic_type: type \"FallbackString\" is already registered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型列表（使用 .pt 文件）\n",
        "models = [\n",
        "    \"yolo147.yaml\",\n",
        "    \"yolo11n.yaml\",\n",
        "    \"yolo12n.yaml\",\n",
        "    \"yolo142n.yaml\",\n",
        "    \"yolo145n.yaml\",\n",
        "    \"yolo146n.yaml\",\n",
        "    \"yolo1471n.yaml\",\n",
        "    \"yolo1472n.yaml\",\n",
        "    \"yolo1473n.yaml\",\n",
        "    \"yolo148.yaml\",\n",
        "    \"yolo149.yaml\"\n",
        "]\n",
        "\n",
        "# 循环执行 yolo 导出命令\n",
        "for model in models:\n",
        "    print(f\"执行命令: !yolo export model={model} format=engine half=True\")\n",
        "    !yolo export model={model} format=engine half=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mMQys7b14zy",
        "outputId": "fe4f7803-41f5-4c0e-8860-7d03870bd03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "执行命令: !yolo export model=yolo147.yaml format=engine half=True\n",
            "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "💡 ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "YOLO147 summary (fused): 137 layers, 2,766,976 parameters, 0 gradients, 6.34 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo147.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.59', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 4.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 11.1s, saved as 'yolo147.onnx' (10.7 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:14:59] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 695, GPU 252 (MiB)\n",
            "[08/31/2025-16:15:06] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1293, GPU 254 (MiB)\n",
            "[08/31/2025-16:15:06] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:15:06] [TRT] [I] Input filename:   yolo147.onnx\n",
            "[08/31/2025-16:15:06] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:15:06] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:15:06] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:15:06] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:15:06] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:15:06] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:15:06] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:15:06] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo147.engine\n",
            "[08/31/2025-16:15:06] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:15:08] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:15:08] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:18:18] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:22:01] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:22:06] [TRT] [I] Total Host Persistent Memory: 606992 bytes\n",
            "[08/31/2025-16:22:06] [TRT] [I] Total Device Persistent Memory: 646144 bytes\n",
            "[08/31/2025-16:22:06] [TRT] [I] Max Scratch Memory: 1382400 bytes\n",
            "[08/31/2025-16:22:06] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 165 steps to complete.\n",
            "[08/31/2025-16:22:06] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 13.7181ms to assign 9 blocks to 165 nodes requiring 11264000 bytes.\n",
            "[08/31/2025-16:22:06] [TRT] [I] Total Activation Memory: 11264000 bytes\n",
            "[08/31/2025-16:22:06] [TRT] [I] Total Weights Memory: 5574434 bytes\n",
            "[08/31/2025-16:22:06] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:22:06] [TRT] [I] Engine generation completed in 418.275 seconds.\n",
            "[08/31/2025-16:22:06] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 439.2s, saved as 'yolo147.engine' (8.1 MB)\n",
            "\n",
            "Export complete (441.4s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo147.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo147.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo11n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.48 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 4.4s, saved as 'yolo11n.onnx' (10.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:22:20] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 679, GPU 252 (MiB)\n",
            "[08/31/2025-16:22:21] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1276, GPU 254 (MiB)\n",
            "[08/31/2025-16:22:21] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:22:21] [TRT] [I] Input filename:   yolo11n.onnx\n",
            "[08/31/2025-16:22:21] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:22:21] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:22:21] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:22:21] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:22:21] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:22:21] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:22:21] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:22:21] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo11n.engine\n",
            "[08/31/2025-16:22:21] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:22:22] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:22:22] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:26:25] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:29:43] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:29:47] [TRT] [I] Total Host Persistent Memory: 497072 bytes\n",
            "[08/31/2025-16:29:47] [TRT] [I] Total Device Persistent Memory: 742400 bytes\n",
            "[08/31/2025-16:29:47] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[08/31/2025-16:29:47] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 165 steps to complete.\n",
            "[08/31/2025-16:29:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 21.4198ms to assign 10 blocks to 165 nodes requiring 10957312 bytes.\n",
            "[08/31/2025-16:29:47] [TRT] [I] Total Activation Memory: 10956800 bytes\n",
            "[08/31/2025-16:29:47] [TRT] [I] Total Weights Memory: 5285892 bytes\n",
            "[08/31/2025-16:29:47] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:29:47] [TRT] [I] Engine generation completed in 444.689 seconds.\n",
            "[08/31/2025-16:29:47] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 453.5s, saved as 'yolo11n.engine' (8.0 MB)\n",
            "\n",
            "Export complete (454.5s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo11n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo12n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO12n summary (fused): 159 layers, 2,590,824 parameters, 0 gradients, 6.49 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo12n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.3s, saved as 'yolo12n.onnx' (10.1 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:29:57] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 685, GPU 252 (MiB)\n",
            "[08/31/2025-16:29:58] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1282, GPU 254 (MiB)\n",
            "[08/31/2025-16:29:58] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:29:58] [TRT] [I] Input filename:   yolo12n.onnx\n",
            "[08/31/2025-16:29:58] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:29:58] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:29:58] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:29:58] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:29:58] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:29:58] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:29:58] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:29:58] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo12n.engine\n",
            "[08/31/2025-16:29:58] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:30:00] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:30:00] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:31:52] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:37:47] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:37:52] [TRT] [I] Total Host Persistent Memory: 692560 bytes\n",
            "[08/31/2025-16:37:52] [TRT] [I] Total Device Persistent Memory: 801280 bytes\n",
            "[08/31/2025-16:37:52] [TRT] [I] Max Scratch Memory: 5529600 bytes\n",
            "[08/31/2025-16:37:52] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 246 steps to complete.\n",
            "[08/31/2025-16:37:52] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 42.6972ms to assign 12 blocks to 246 nodes requiring 13415424 bytes.\n",
            "[08/31/2025-16:37:52] [TRT] [I] Total Activation Memory: 13414400 bytes\n",
            "[08/31/2025-16:37:52] [TRT] [I] Total Weights Memory: 5278498 bytes\n",
            "[08/31/2025-16:37:52] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:37:52] [TRT] [I] Engine generation completed in 472.187 seconds.\n",
            "[08/31/2025-16:37:52] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 478.0s, saved as 'yolo12n.engine' (9.1 MB)\n",
            "\n",
            "Export complete (479.7s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo12n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo12n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo142n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO142n summary (fused): 133 layers, 2,577,792 parameters, 0 gradients, 6.29 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo142n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.6s, saved as 'yolo142n.onnx' (10.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:37:59] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 677, GPU 252 (MiB)\n",
            "[08/31/2025-16:38:00] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1274, GPU 254 (MiB)\n",
            "[08/31/2025-16:38:00] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:38:00] [TRT] [I] Input filename:   yolo142n.onnx\n",
            "[08/31/2025-16:38:00] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:38:00] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:38:00] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:38:00] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:38:00] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:38:00] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:38:00] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:38:00] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo142n.engine\n",
            "[08/31/2025-16:38:00] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:38:02] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:38:02] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:41:49] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:44:54] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:44:58] [TRT] [I] Total Host Persistent Memory: 563424 bytes\n",
            "[08/31/2025-16:44:58] [TRT] [I] Total Device Persistent Memory: 620544 bytes\n",
            "[08/31/2025-16:44:58] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[08/31/2025-16:44:58] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 154 steps to complete.\n",
            "[08/31/2025-16:44:58] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 18.0246ms to assign 10 blocks to 154 nodes requiring 13030912 bytes.\n",
            "[08/31/2025-16:44:58] [TRT] [I] Total Activation Memory: 13030400 bytes\n",
            "[08/31/2025-16:44:58] [TRT] [I] Total Weights Memory: 5252360 bytes\n",
            "[08/31/2025-16:44:58] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:44:58] [TRT] [I] Engine generation completed in 416.145 seconds.\n",
            "[08/31/2025-16:44:58] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 420.7s, saved as 'yolo142n.engine' (7.5 MB)\n",
            "\n",
            "Export complete (421.6s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo142n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo142n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo145n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO145n summary (fused): 136 layers, 2,700,864 parameters, 0 gradients, 6.29 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo145n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.7s, saved as 'yolo145n.onnx' (10.5 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:45:06] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 691, GPU 252 (MiB)\n",
            "[08/31/2025-16:45:07] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1288, GPU 254 (MiB)\n",
            "[08/31/2025-16:45:07] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:45:07] [TRT] [I] Input filename:   yolo145n.onnx\n",
            "[08/31/2025-16:45:07] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:45:07] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:45:07] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:45:07] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:45:07] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:45:07] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:45:07] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:45:07] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo145n.engine\n",
            "[08/31/2025-16:45:07] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:45:08] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:45:08] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:48:50] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:51:54] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:51:58] [TRT] [I] Total Host Persistent Memory: 586896 bytes\n",
            "[08/31/2025-16:51:58] [TRT] [I] Total Device Persistent Memory: 558592 bytes\n",
            "[08/31/2025-16:51:58] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[08/31/2025-16:51:58] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 165 steps to complete.\n",
            "[08/31/2025-16:51:58] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 12.6733ms to assign 11 blocks to 165 nodes requiring 11469824 bytes.\n",
            "[08/31/2025-16:51:58] [TRT] [I] Total Activation Memory: 11468800 bytes\n",
            "[08/31/2025-16:51:58] [TRT] [I] Total Weights Memory: 5446412 bytes\n",
            "[08/31/2025-16:51:58] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:51:58] [TRT] [I] Engine generation completed in 409.792 seconds.\n",
            "[08/31/2025-16:51:58] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 414.6s, saved as 'yolo145n.engine' (8.2 MB)\n",
            "\n",
            "Export complete (415.5s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo145n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo145n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo146n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO146n summary (fused): 142 layers, 2,653,376 parameters, 0 gradients, 6.25 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo146n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.7s, saved as 'yolo146n.onnx' (10.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:52:06] [TRT] [I] [MemUsageChange] Init CUDA: CPU -1, GPU +0, now: CPU 690, GPU 252 (MiB)\n",
            "[08/31/2025-16:52:07] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1287, GPU 254 (MiB)\n",
            "[08/31/2025-16:52:07] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:52:07] [TRT] [I] Input filename:   yolo146n.onnx\n",
            "[08/31/2025-16:52:07] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:52:07] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:52:07] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:52:07] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:52:07] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:52:07] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:52:07] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:52:07] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo146n.engine\n",
            "[08/31/2025-16:52:07] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:52:08] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:52:08] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-16:55:45] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-16:58:45] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-16:58:49] [TRT] [I] Total Host Persistent Memory: 596832 bytes\n",
            "[08/31/2025-16:58:49] [TRT] [I] Total Device Persistent Memory: 611840 bytes\n",
            "[08/31/2025-16:58:49] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[08/31/2025-16:58:49] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 160 steps to complete.\n",
            "[08/31/2025-16:58:49] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 12.6337ms to assign 11 blocks to 160 nodes requiring 13543424 bytes.\n",
            "[08/31/2025-16:58:49] [TRT] [I] Total Activation Memory: 13542400 bytes\n",
            "[08/31/2025-16:58:49] [TRT] [I] Total Weights Memory: 5401478 bytes\n",
            "[08/31/2025-16:58:49] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-16:58:49] [TRT] [I] Engine generation completed in 401.021 seconds.\n",
            "[08/31/2025-16:58:49] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 405.5s, saved as 'yolo146n.engine' (8.0 MB)\n",
            "\n",
            "Export complete (406.5s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo146n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo146n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo1471n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO1471n summary (fused): 148 layers, 2,788,096 parameters, 0 gradients, 6.43 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo1471n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.0s, saved as 'yolo1471n.onnx' (10.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-16:58:57] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 692, GPU 254 (MiB)\n",
            "[08/31/2025-16:58:58] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1290, GPU 256 (MiB)\n",
            "[08/31/2025-16:58:58] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-16:58:58] [TRT] [I] Input filename:   yolo1471n.onnx\n",
            "[08/31/2025-16:58:58] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-16:58:58] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-16:58:58] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-16:58:58] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-16:58:58] [TRT] [I] Domain:           \n",
            "[08/31/2025-16:58:58] [TRT] [I] Model version:    0\n",
            "[08/31/2025-16:58:58] [TRT] [I] Doc string:       \n",
            "[08/31/2025-16:58:58] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo1471n.engine\n",
            "[08/31/2025-16:58:58] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:58:59] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-16:58:59] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-17:02:08] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-17:05:33] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-17:05:38] [TRT] [I] Total Host Persistent Memory: 660864 bytes\n",
            "[08/31/2025-17:05:38] [TRT] [I] Total Device Persistent Memory: 625664 bytes\n",
            "[08/31/2025-17:05:38] [TRT] [I] Max Scratch Memory: 1382400 bytes\n",
            "[08/31/2025-17:05:38] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 207 steps to complete.\n",
            "[08/31/2025-17:05:38] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 17.1159ms to assign 11 blocks to 207 nodes requiring 11572224 bytes.\n",
            "[08/31/2025-17:05:38] [TRT] [I] Total Activation Memory: 11571200 bytes\n",
            "[08/31/2025-17:05:38] [TRT] [I] Total Weights Memory: 5613858 bytes\n",
            "[08/31/2025-17:05:38] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-17:05:38] [TRT] [I] Engine generation completed in 398.826 seconds.\n",
            "[08/31/2025-17:05:38] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 403.7s, saved as 'yolo1471n.engine' (8.3 MB)\n",
            "\n",
            "Export complete (405.0s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo1471n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo1471n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo1472n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO1472n summary (fused): 148 layers, 2,788,096 parameters, 0 gradients, 6.43 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo1472n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.8s, saved as 'yolo1472n.onnx' (10.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-17:05:45] [TRT] [I] [MemUsageChange] Init CUDA: CPU -1, GPU +0, now: CPU 693, GPU 254 (MiB)\n",
            "[08/31/2025-17:05:47] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1290, GPU 256 (MiB)\n",
            "[08/31/2025-17:05:47] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-17:05:47] [TRT] [I] Input filename:   yolo1472n.onnx\n",
            "[08/31/2025-17:05:47] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-17:05:47] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-17:05:47] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-17:05:47] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-17:05:47] [TRT] [I] Domain:           \n",
            "[08/31/2025-17:05:47] [TRT] [I] Model version:    0\n",
            "[08/31/2025-17:05:47] [TRT] [I] Doc string:       \n",
            "[08/31/2025-17:05:47] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo1472n.engine\n",
            "[08/31/2025-17:05:47] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:05:48] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:05:48] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-17:08:55] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-17:12:26] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-17:12:31] [TRT] [I] Total Host Persistent Memory: 675440 bytes\n",
            "[08/31/2025-17:12:31] [TRT] [I] Total Device Persistent Memory: 367616 bytes\n",
            "[08/31/2025-17:12:31] [TRT] [I] Max Scratch Memory: 1382400 bytes\n",
            "[08/31/2025-17:12:31] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 236 steps to complete.\n",
            "[08/31/2025-17:12:31] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 23.3132ms to assign 12 blocks to 236 nodes requiring 11572736 bytes.\n",
            "[08/31/2025-17:12:31] [TRT] [I] Total Activation Memory: 11571200 bytes\n",
            "[08/31/2025-17:12:31] [TRT] [I] Total Weights Memory: 5641506 bytes\n",
            "[08/31/2025-17:12:31] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-17:12:31] [TRT] [I] Engine generation completed in 402.84 seconds.\n",
            "[08/31/2025-17:12:31] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 408.2s, saved as 'yolo1472n.engine' (8.4 MB)\n",
            "\n",
            "Export complete (409.1s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo1472n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo1472n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo1473n.yaml format=engine half=True\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO1473n summary (fused): 170 layers, 2,788,096 parameters, 0 gradients, 6.43 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo1473n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.8s, saved as 'yolo1473n.onnx' (10.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-17:12:39] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 725, GPU 254 (MiB)\n",
            "[08/31/2025-17:12:40] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1322, GPU 256 (MiB)\n",
            "[08/31/2025-17:12:40] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-17:12:40] [TRT] [I] Input filename:   yolo1473n.onnx\n",
            "[08/31/2025-17:12:40] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-17:12:40] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-17:12:40] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-17:12:40] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-17:12:40] [TRT] [I] Domain:           \n",
            "[08/31/2025-17:12:40] [TRT] [I] Model version:    0\n",
            "[08/31/2025-17:12:40] [TRT] [I] Doc string:       \n",
            "[08/31/2025-17:12:40] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo1473n.engine\n",
            "[08/31/2025-17:12:40] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:12:42] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:12:42] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-17:15:46] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-17:19:18] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-17:19:23] [TRT] [I] Total Host Persistent Memory: 654352 bytes\n",
            "[08/31/2025-17:19:23] [TRT] [I] Total Device Persistent Memory: 353792 bytes\n",
            "[08/31/2025-17:19:23] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[08/31/2025-17:19:23] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 186 steps to complete.\n",
            "[08/31/2025-17:19:23] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 14.8306ms to assign 11 blocks to 186 nodes requiring 11674624 bytes.\n",
            "[08/31/2025-17:19:23] [TRT] [I] Total Activation Memory: 11673600 bytes\n",
            "[08/31/2025-17:19:23] [TRT] [I] Total Weights Memory: 5912868 bytes\n",
            "[08/31/2025-17:19:23] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-17:19:23] [TRT] [I] Engine generation completed in 401.663 seconds.\n",
            "[08/31/2025-17:19:23] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 407.0s, saved as 'yolo1473n.engine' (8.6 MB)\n",
            "\n",
            "Export complete (408.0s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo1473n.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo1473n.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo148.yaml format=engine half=True\n",
            "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO148 summary (fused): 139 layers, 2,644,096 parameters, 0 gradients, 6.19 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo148.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.7s, saved as 'yolo148.onnx' (10.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-17:19:30] [TRT] [I] [MemUsageChange] Init CUDA: CPU -1, GPU +0, now: CPU 678, GPU 252 (MiB)\n",
            "[08/31/2025-17:19:31] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1275, GPU 254 (MiB)\n",
            "[08/31/2025-17:19:31] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-17:19:31] [TRT] [I] Input filename:   yolo148.onnx\n",
            "[08/31/2025-17:19:31] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-17:19:31] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-17:19:31] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-17:19:31] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-17:19:31] [TRT] [I] Domain:           \n",
            "[08/31/2025-17:19:31] [TRT] [I] Model version:    0\n",
            "[08/31/2025-17:19:31] [TRT] [I] Doc string:       \n",
            "[08/31/2025-17:19:31] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo148.engine\n",
            "[08/31/2025-17:19:31] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:19:33] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:19:33] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-17:22:39] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-17:26:16] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-17:26:20] [TRT] [I] Total Host Persistent Memory: 609536 bytes\n",
            "[08/31/2025-17:26:20] [TRT] [I] Total Device Persistent Memory: 613376 bytes\n",
            "[08/31/2025-17:26:20] [TRT] [I] Max Scratch Memory: 1382400 bytes\n",
            "[08/31/2025-17:26:20] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 166 steps to complete.\n",
            "[08/31/2025-17:26:20] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 12.4769ms to assign 9 blocks to 166 nodes requiring 13337600 bytes.\n",
            "[08/31/2025-17:26:20] [TRT] [I] Total Activation Memory: 13337600 bytes\n",
            "[08/31/2025-17:26:20] [TRT] [I] Total Weights Memory: 5379334 bytes\n",
            "[08/31/2025-17:26:20] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-17:26:20] [TRT] [I] Engine generation completed in 407.393 seconds.\n",
            "[08/31/2025-17:26:20] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 412.0s, saved as 'yolo148.engine' (7.8 MB)\n",
            "\n",
            "Export complete (412.9s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo148.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo148.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n",
            "执行命令: !yolo export model=yolo149.yaml format=engine half=True\n",
            "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO149 summary (fused): 133 layers, 2,577,792 parameters, 0 gradients, 6.29 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo149.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.6s, saved as 'yolo149.onnx' (10.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
            "[08/31/2025-17:26:27] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 677, GPU 252 (MiB)\n",
            "[08/31/2025-17:26:28] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +453, GPU +2, now: CPU 1274, GPU 254 (MiB)\n",
            "[08/31/2025-17:26:28] [TRT] [I] ----------------------------------------------------------------\n",
            "[08/31/2025-17:26:28] [TRT] [I] Input filename:   yolo149.onnx\n",
            "[08/31/2025-17:26:28] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[08/31/2025-17:26:28] [TRT] [I] Opset version:    19\n",
            "[08/31/2025-17:26:28] [TRT] [I] Producer name:    pytorch\n",
            "[08/31/2025-17:26:28] [TRT] [I] Producer version: 2.8.0\n",
            "[08/31/2025-17:26:28] [TRT] [I] Domain:           \n",
            "[08/31/2025-17:26:28] [TRT] [I] Model version:    0\n",
            "[08/31/2025-17:26:28] [TRT] [I] Doc string:       \n",
            "[08/31/2025-17:26:28] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo149.engine\n",
            "[08/31/2025-17:26:28] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:26:30] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[08/31/2025-17:26:30] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[08/31/2025-17:30:22] [TRT] [I] Compiler backend is used during engine build.\n",
            "[08/31/2025-17:33:29] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[08/31/2025-17:33:33] [TRT] [I] Total Host Persistent Memory: 570256 bytes\n",
            "[08/31/2025-17:33:33] [TRT] [I] Total Device Persistent Memory: 492032 bytes\n",
            "[08/31/2025-17:33:33] [TRT] [I] Max Scratch Memory: 1382400 bytes\n",
            "[08/31/2025-17:33:33] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 166 steps to complete.\n",
            "[08/31/2025-17:33:33] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 13.5153ms to assign 10 blocks to 166 nodes requiring 10957312 bytes.\n",
            "[08/31/2025-17:33:33] [TRT] [I] Total Activation Memory: 10956800 bytes\n",
            "[08/31/2025-17:33:33] [TRT] [I] Total Weights Memory: 5236998 bytes\n",
            "[08/31/2025-17:33:33] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[08/31/2025-17:33:33] [TRT] [I] Engine generation completed in 422.573 seconds.\n",
            "[08/31/2025-17:33:33] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 261 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ✅ 427.9s, saved as 'yolo149.engine' (8.0 MB)\n",
            "\n",
            "Export complete (428.8s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo149.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo149.engine imgsz=640 data=None half \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=\"yolov12n.yaml\" format=engine half=True"
      ],
      "metadata": {
        "id": "T_2dSjbuui8L",
        "outputId": "08fe4484-7608-465c-f3b9-9e058b3bed91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics 8.3.63 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12n summary (fused): 376 layers, 2,542,440 parameters, 0 gradients, 6.0 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov12n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.67...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/cfg/__init__.py\", line 983, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/model.py\", line 740, in export\n",
            "    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/exporter.py\", line 394, in __call__\n",
            "    f[1], _ = self.export_engine(dla=dla)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/exporter.py\", line 171, in outer_func\n",
            "    f, model = inner_func(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/exporter.py\", line 803, in export_engine\n",
            "    f_onnx, _ = self.export_onnx()  # run before TRT import https://github.com/ultralytics/ultralytics/issues/7016\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/exporter.py\", line 171, in outer_func\n",
            "    f, model = inner_func(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12/ultralytics/engine/exporter.py\", line 530, in export_onnx\n",
            "    model_onnx = onnxslim.slim(model_onnx)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxslim/cli/_main.py\", line 103, in slim\n",
            "    model = shape_infer(model)\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxslim/core/__init__.py\", line 127, in shape_infer\n",
            "    model = SymbolicShapeInference.infer_shapes(model, auto_merge=AUTO_MERGE)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxslim/third_party/symbolic_shape_infer.py\", line 3157, in infer_shapes\n",
            "    all_shapes_inferred = symbolic_shape_inference._infer_impl()\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxslim/third_party/symbolic_shape_infer.py\", line 2911, in _infer_impl\n",
            "    self._onnx_infer_single_node(node)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxslim/third_party/symbolic_shape_infer.py\", line 577, in _onnx_infer_single_node\n",
            "    model = shape_inference.infer_shapes(model)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/shape_inference.py\", line 52, in infer_shapes\n",
            "    inferred_model_str = C.infer_shapes(\n",
            "                         ^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLO11 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO11 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO11 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib'\n",
        "print(os.environ['LD_LIBRARY_PATH'])\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.benchmarks import benchmark\n",
        "from ultralytics.utils.benchmarks import ProfileModels\n",
        "\n",
        "profiler = ProfileModels([\"yolo147.yaml\", \"yolo11n.yaml\", \"yolo12n.yaml\", \"yolo142n.yaml\", \"yolo145n.yaml\",\"yolo146n.yaml\", \"yolo1471n.yaml\", \"yolo1472n.yaml\", \"yolo1473n.yaml\", \"yolo148.yaml\", \"yolo149.yaml\"], imgsz=640, device='cuda:6')\n",
        "profiler.run()\n"
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783aaf80-ac32-4c8a-ad34-05c456ca681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/lib64-nvidia:/content/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib\n",
            "Ultralytics 8.3.180 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Profiling: ['yolo147n.onnx']\n",
            "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Loading yolo147n.engine for TensorRT inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "yolo147n.engine: 100%|██████████| 6864/6864 [00:39<00:00, 172.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnxruntime'] not found, attempting AutoUpdate...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "yolo147n.onnx: 100%|██████████| 103/103 [01:11<00:00,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "| Model | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU (Intel Xeon 2.00GHz) ONNX<br>(ms) | Speed<br><sup>Tesla T4 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "|-------|-----------------------|----------------------|-----------------------------------------------------|-----------------------------------------|--------------------|-------------------|\n",
            "| yolo147n           | 640 | - | 610.899±47.701 ms | 3.332±0.037 ms | 0.0 | 0.0 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model/name': 'yolo147n',\n",
              "  'model/parameters': 0.0,\n",
              "  'model/GFLOPs': 0.0,\n",
              "  'model/speed_ONNX(ms)': np.float64(610.899),\n",
              "  'model/speed_TensorRT(ms)': np.float64(3.332)}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib'\n",
        "print(os.environ['LD_LIBRARY_PATH'])\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.benchmarks import benchmark\n",
        "from ultralytics.utils.benchmarks import ProfileModels\n",
        "\n",
        "profiler = ProfileModels([\"yolov12n.yaml\", \"yolo11n.yaml\"], imgsz=640, half=True)\n",
        "profiler.profile()"
      ],
      "metadata": {
        "id": "cQuKTg8l0MsS",
        "outputId": "3177adb4-7489-4a21-85e2-dfccc66bc9f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/lib64-nvidia:/content/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib:/content/gdrive/MyDrive/TensorRT-10.10.0.31/lib\n",
            "Profiling: ['yolo11n.yaml', 'yolov12n.yaml']\n",
            "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 2,616,232 gradients, 6.5 GFLOPs\n",
            "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 2,616,232 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.67...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.1s, saved as 'yolo11n.onnx' (10.2 MB)\n",
            "\n",
            "Export complete (1.1s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11n.onnx imgsz=640 data=None  \n",
            "Visualize:       https://netron.app\n",
            "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Loading yolo11n.engine for TensorRT inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "yolo11n.engine: 100%|██████████| 10358/10358 [00:39<00:00, 258.96it/s]\n",
            "yolo11n.onnx: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv12n summary (fused): 376 layers, 2,542,440 parameters, 2,542,424 gradients, 6.0 GFLOPs\n",
            "YOLOv12n summary (fused): 376 layers, 2,542,440 parameters, 2,542,424 gradients, 6.0 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov12n.yaml' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.67...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.1s, saved as 'yolov12n.onnx' (10.0 MB)\n",
            "\n",
            "Export complete (2.2s)\n",
            "Results saved to \u001b[1m/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov12n.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov12n.onnx imgsz=640 data=None  \n",
            "Visualize:       https://netron.app\n",
            "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Loading yolov12n.engine for TensorRT inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "yolov12n.engine: 100%|██████████| 9704/9704 [00:44<00:00, 217.17it/s]\n",
            "yolov12n.onnx: 100%|██████████| 100/100 [01:28<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "| Model | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU (Intel Xeon 2.20GHz) ONNX<br>(ms) | Speed<br><sup>Tesla T4 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "|-------|-----------------------|----------------------|-----------------------------------------------------|-----------------------------------------|--------------------|-------------------|\n",
            "| yolo11n            | 640 | - | 629.9±37.3 ms | 1.7±0.0 ms | 2.6 | 6.5 |\n",
            "| yolov12n           | 640 | - | 790.9±38.6 ms | 2.4±0.0 ms | 2.5 | 6.0 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model/name': 'yolo11n',\n",
              "  'model/parameters': 2616248,\n",
              "  'model/GFLOPs': 6.484,\n",
              "  'model/speed_ONNX(ms)': np.float64(629.88),\n",
              "  'model/speed_TensorRT(ms)': np.float64(1.698)},\n",
              " {'model/name': 'yolov12n',\n",
              "  'model/parameters': 2542440,\n",
              "  'model/GFLOPs': 5.989,\n",
              "  'model/speed_ONNX(ms)': np.float64(790.923),\n",
              "  'model/speed_TensorRT(ms)': np.float64(2.373)}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip uninstall flash-attn\n",
        "! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "Luul7kmZARS9",
        "outputId": "75b0806c-4012-4e4e-88fd-af3e1262878f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flash_attn 2.8.3\n",
            "Uninstalling flash_attn-2.8.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn-2.8.3.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn/*\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.12/dist-packages/hopper/*\n",
            "Proceed (Y/n)? y\n",
            "y\n",
            "  Successfully uninstalled flash_attn-2.8.3\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ninja"
      ],
      "metadata": {
        "id": "a90woa9MDZF3",
        "outputId": "edb9b8e5-cfa1-44ce-fa53-691ed5325928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Installing collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Dao-AILab/flash-attention\n",
        "%cd flash_attn\n",
        "! pip uninstall flash-attn\n",
        "! pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "ihh9Fe5GE7ba",
        "outputId": "faccef97-3c87-4ffb-d6c6-4eeae72bdd20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flash-attention'...\n",
            "remote: Enumerating objects: 10801, done.\u001b[K\n",
            "remote: Counting objects: 100% (325/325), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 10801 (delta 289), reused 237 (delta 237), pack-reused 10476 (from 3)\u001b[K\n",
            "Receiving objects: 100% (10801/10801), 10.01 MiB | 16.29 MiB/s, done.\n",
            "Resolving deltas: 100% (8284/8284), done.\n",
            "Updating files: 100% (975/975), done.\n",
            "[Errno 2] No such file or directory: 'flash_attn'\n",
            "/content/gdrive/MyDrive/Colab Notebooks/TensorRT Colab Test/yolov12\n",
            "Found existing installation: flash_attn 2.8.3\n",
            "Uninstalling flash_attn-2.8.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn-2.8.3.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn/*\n",
            "    /usr/local/lib/python3.12/dist-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so\n",
            "    /usr/local/lib/python3.12/dist-packages/hopper/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled flash_attn-2.8.3\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pytest flash-attention/tests/test_flash_attn.py"
      ],
      "metadata": {
        "id": "rhKUUqQhF7_T",
        "outputId": "4f26af8c-df8f-449b-a9d3-c11f5b4b8692",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-False-False-True-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-32-False-True-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-False-True-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-False-True-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-False-True-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-32-True-False-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-False-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-False-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-False-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-True-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = True\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-True-False-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = True\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-32-True-True-True-False-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = True, alibi = True\n",
            "deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_______ test_flash_attn_qkvpacked[0.0-128-32-True-True-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 32, dropout_p = 0.0, causal = True, local = True, alibi = True\n",
            "deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -8.3740e-01,\n",
            "            1.3506e+00, -2.8784e-01],\n",
            "          [...734e+00,  6.0107e-01,  ...,  1.4873e+00,\n",
            "            2.9297e-01,  2.2522e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 6.6992e-01, -6.6650e-01,  1.9092e-01,  ...,  1.9092e+00,\n",
            "           -8.1885e-01, -6.7627e-01],\n",
            "          [...504e+00, -8.9014e-01,  ...,  1.4375e+00,\n",
            "           -6.2012e-01, -1.5146e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[-7.8809e-01, -1.4102e+00, -1.9116e-01,  ..., -4.7217e-01,\n",
            "            7.3877e-01, -1.5791e+00],\n",
            "          [...046e-01, -1.3379e+00,  ..., -1.1299e+00,\n",
            "            2.0254e+00,  1.9951e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.1767766952966369, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2362, 0.0403, 0.0657, 0.2458, 0.2091, 0.2191, 0.0698, 0.2708, 0.1170],\n",
            "        [0.1853, 0.1504, 0.0699, 0.2...69, 0.0187],\n",
            "        [0.1825, 0.1157, 0.2241, 0.2668, 0.0443, 0.1083, 0.2662, 0.0314, 0.1210]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-40-False-False-False-False-dtype0] _____\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-40-False-False-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-40-False-False-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-False-False-True-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-40-False-True-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-False-True-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-False-True-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-False-True-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-40-True-False-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-False-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-False-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-False-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = False\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-True-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = True\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-True-False-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = True\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-40-True-True-True-False-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = True, alibi = True\n",
            "deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_______ test_flash_attn_qkvpacked[0.0-128-40-True-True-True-True-dtype0] _______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 40, dropout_p = 0.0, causal = True, local = True, alibi = True\n",
            "deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-9.2480e-01, -4.2529e-01, -2.6445e+00,  ..., -5.0385e-02,\n",
            "            5.4346e-01,  1.5146e+00],\n",
            "          [...725e-01,  1.7749e-01,  ..., -9.8389e-01,\n",
            "           -2.4976e-01,  1.3770e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[-8.9783e-02,  1.1101e-02, -3.3740e-01,  ...,  6.1670e-01,\n",
            "           -1.2363e+00, -3.2544e-01],\n",
            "          [...912e+00, -9.6973e-01,  ...,  1.8906e+00,\n",
            "            3.3813e-01,  1.2529e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 4.7852e-01,  4.3652e-01, -5.4688e-01,  ...,  1.4141e+00,\n",
            "           -1.2178e+00, -2.7051e+00],\n",
            "          [...429e-02,  4.9072e-01,  ...,  2.8145e+00,\n",
            "            2.0203e-01,  5.7812e-01]]]], device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.15811388300841897, causal = True\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.2804, 0.2462, 0.0049, 0.0567, 0.2885, 0.0131, 0.1455, 0.2345, 0.1402],\n",
            "        [0.1011, 0.0322, 0.2540, 0.0...58, 0.1924],\n",
            "        [0.2674, 0.2839, 0.0834, 0.0877, 0.0773, 0.1658, 0.2894, 0.1761, 0.1524]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-59-False-False-False-False-dtype0] _____\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 59, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-0.9248, -0.4253, -2.6445,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9048,  0.2837,  0.1210,  ...,  0...          [ 0.1888, -0.5635,  0.1703,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [...719e-01, -1.0723e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 1.5400,  0.2717, -1.2051,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3911,  0.9365,  0.6909,  ...,  0...          [ 0.1813,  0.2251,  0.0681,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-59-False-False-False-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 59, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = False, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-0.9248, -0.4253, -2.6445,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9048,  0.2837,  0.1210,  ...,  0...          [ 0.1888, -0.5635,  0.1703,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [...719e-01, -1.0723e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 1.5400,  0.2717, -1.2051,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3911,  0.9365,  0.6909,  ...,  0...          [ 0.1813,  0.2251,  0.0681,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-59-False-False-True-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 59, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = True, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-0.9248, -0.4253, -2.6445,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9048,  0.2837,  0.1210,  ...,  0...          [ 0.1888, -0.5635,  0.1703,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [...719e-01, -1.0723e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 1.5400,  0.2717, -1.2051,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3911,  0.9365,  0.6909,  ...,  0...          [ 0.1813,  0.2251,  0.0681,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.1351, 0.2530, 0.2736, 0.0600, 0.0627, 0.1688, 0.2738, 0.1595, 0.0367],\n",
            "        [0.2087, 0.1090, 0.1867, 0.1...94, 0.2681],\n",
            "        [0.0182, 0.1767, 0.1694, 0.2761, 0.0562, 0.0894, 0.2751, 0.0057, 0.2425]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m______ test_flash_attn_qkvpacked[0.0-128-59-False-False-True-True-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 59, dropout_p = 0.0, causal = False, local = False\n",
            "alibi = True, deterministic = True, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-0.9248, -0.4253, -2.6445,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9048,  0.2837,  0.1210,  ...,  0...          [ 0.1888, -0.5635,  0.1703,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [...719e-01, -1.0723e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 1.5400,  0.2717, -1.2051,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3911,  0.9365,  0.6909,  ...,  0...          [ 0.1813,  0.2251,  0.0681,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size_left = -1, window_size_right = -1, softcap = 0.0\n",
            "alibi_slopes = tensor([[0.1351, 0.2530, 0.2736, 0.0600, 0.0627, 0.1688, 0.2738, 0.1595, 0.0367],\n",
            "        [0.2087, 0.1090, 0.1867, 0.1...94, 0.2681],\n",
            "        [0.0182, 0.1767, 0.1694, 0.2761, 0.0562, 0.0894, 0.2751, 0.0057, 0.2425]],\n",
            "       device='cuda:0')\n",
            "return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[31m\u001b[1m_____ test_flash_attn_qkvpacked[0.0-128-59-False-True-False-False-dtype0] ______\u001b[0m\n",
            "\n",
            "seqlen = 128, d = 59, dropout_p = 0.0, causal = False, local = True\n",
            "alibi = False, deterministic = False, dtype = torch.float16\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ([torch.float16] \u001b[94mif\u001b[39;49;00m is_sm75 \u001b[94melse\u001b[39;49;00m [torch.float16, torch.bfloat16]))\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dtype\", [torch.float16])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdeterministic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"deterministic\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33malibi\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"alibi\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"local\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"causal\", [False])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m, \u001b[94m40\u001b[39;49;00m, \u001b[94m59\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m, \u001b[94m80\u001b[39;49;00m, \u001b[94m96\u001b[39;49;00m, \u001b[94m111\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m160\u001b[39;49;00m, \u001b[94m192\u001b[39;49;00m, \u001b[94m224\u001b[39;49;00m, \u001b[94m256\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('d', [32, 64, 96, 128])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"d\", [64])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize('seqlen', [128, 256, 384, 512, 768, 1024, 2048])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqlen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m97\u001b[39;49;00m, \u001b[94m128\u001b[39;49;00m, \u001b[94m200\u001b[39;49;00m, \u001b[94m384\u001b[39;49;00m, \u001b[94m768\u001b[39;49;00m, \u001b[94m1024\u001b[39;49;00m, \u001b[94m1025\u001b[39;49;00m, \u001b[94m2048\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"seqlen\", [512])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout_p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.17\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m# @pytest.mark.parametrize(\"dropout_p\", [0.0])\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_flash_attn_qkvpacked\u001b[39;49;00m(seqlen, d, dropout_p, causal, local, alibi, deterministic, dtype):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m seqlen >= \u001b[94m2048\u001b[39;49;00m \u001b[95mand\u001b[39;49;00m torch.cuda.get_device_properties(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).total_memory <= \u001b[94m16\u001b[39;49;00m * \u001b[94m2\u001b[39;49;00m**\u001b[94m30\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            pytest.skip()  \u001b[90m# Reference implementation OOM\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# set seed\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        torch.random.manual_seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        batch_size = \u001b[94m4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        nheads = \u001b[94m9\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        window_size = (-\u001b[94m1\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m local \u001b[94melse\u001b[39;49;00m torch.randint(\u001b[94m0\u001b[39;49;00m, seqlen, (\u001b[94m2\u001b[39;49;00m,))\u001b[90m\u001b[39;49;00m\n",
            "        qkv = torch.randn(\u001b[90m\u001b[39;49;00m\n",
            "            batch_size, seqlen, \u001b[94m3\u001b[39;49;00m, nheads, d, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m alibi:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes = torch.rand(batch_size, nheads, device=device, dtype=torch.float32) * \u001b[94m0.3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            attn_bias = attn_bias_from_alibi_slopes(alibi_slopes, seqlen, seqlen, causal=causal)\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes, attn_bias = \u001b[94mNone\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       out, lse, S_dmask = flash_attn_qkvpacked_func(\u001b[90m\u001b[39;49;00m\n",
            "            qkv,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            causal=causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size=window_size,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes=alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            deterministic=deterministic,\u001b[90m\u001b[39;49;00m\n",
            "            return_attn_probs=\u001b[94mTrue\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mflash-attention/tests/test_flash_attn.py\u001b[0m:603: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:1043: in flash_attn_qkvpacked_func\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m FlashAttnQKVPackedFunc.apply(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m:576: in apply\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().apply(*args, **kwargs)  \u001b[90m# type: ignore[misc]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:464: in forward\n",
            "    \u001b[0mout_padded, softmax_lse, S_dmask, rng_state =  _wrapped_flash_attn_forward(\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1243: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._op(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:111: in autograd_impl\n",
            "    \u001b[0mresult = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py\u001b[0m:40: in forward_no_grad\n",
            "    \u001b[0mresult = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:836: in redispatch\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._handle.redispatch_boxed(keyset, *args, **kwargs)  \u001b[90m# type: ignore[return-value]\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:344: in backend_impl\n",
            "    \u001b[0mresult = \u001b[96mself\u001b[39;49;00m._backend_fns[device_type](*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m:53: in inner\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m disable_fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m:929: in _fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py\u001b[0m:377: in wrapped_fn\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m fn(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "q = tensor([[[[-0.9248, -0.4253, -2.6445,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.9048,  0.2837,  0.1210,  ...,  0...          [ 0.1888, -0.5635,  0.1703,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "k = tensor([[[[ 8.5107e-01,  4.1919e-01, -3.4644e-01,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [...719e-01, -1.0723e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)\n",
            "v = tensor([[[[ 1.5400,  0.2717, -1.2051,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3911,  0.9365,  0.6909,  ...,  0...          [ 0.1813,  0.2251,  0.0681,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dropout_p = 0.0, softmax_scale = 0.13018891098082386, causal = False\n",
            "window_size_left = 44, window_size_right = 47, softcap = 0.0\n",
            "alibi_slopes = None, return_softmax = False\n",
            "\n",
            "    \u001b[0m\u001b[37m@_torch_custom_op_wrapper\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mflash_attn::_flash_attn_forward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, mutates_args=(), device_types=\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_flash_attn_forward\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "        q: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        k: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        v: torch.Tensor,\u001b[90m\u001b[39;49;00m\n",
            "        dropout_p: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softmax_scale: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        causal: \u001b[96mbool\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_left: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        window_size_right: \u001b[96mint\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        softcap: \u001b[96mfloat\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        alibi_slopes: Optional[torch.Tensor],\u001b[90m\u001b[39;49;00m\n",
            "        return_softmax: \u001b[96mbool\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\u001b[90m\u001b[39;49;00m\n",
            "        q, k, v = [maybe_contiguous(x) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m (q, k, v)]\u001b[90m\u001b[39;49;00m\n",
            ">       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.fwd(\u001b[90m\u001b[39;49;00m\n",
            "            q,\u001b[90m\u001b[39;49;00m\n",
            "            k,\u001b[90m\u001b[39;49;00m\n",
            "            v,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            alibi_slopes,\u001b[90m\u001b[39;49;00m\n",
            "            dropout_p,\u001b[90m\u001b[39;49;00m\n",
            "            softmax_scale,\u001b[90m\u001b[39;49;00m\n",
            "            causal,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_left,\u001b[90m\u001b[39;49;00m\n",
            "            window_size_right,\u001b[90m\u001b[39;49;00m\n",
            "            softcap,\u001b[90m\u001b[39;49;00m\n",
            "            return_softmax,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       RuntimeError: FlashAttention only supports Ampere GPUs or newer.\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m:91: RuntimeError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-32-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-40-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-59-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-64-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-80-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-96-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-111-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-128-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-160-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-192-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-224-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-97-256-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-32-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-False-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-True-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-True-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-40-True-True-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-59-False-False-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-59-False-False-False-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-59-False-False-True-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-59-False-False-True-True-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "\u001b[31mFAILED\u001b[0m flash-attention/tests/test_flash_attn.py::\u001b[1mtest_flash_attn_qkvpacked[0.0-128-59-False-True-False-False-dtype0]\u001b[0m - RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! KeyboardInterrupt !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\u001b[1m\u001b[31m/usr/lib/python3.12/warnings.py\u001b[0m:505: KeyboardInterrupt\n",
            "\u001b[33m(to show a full traceback on KeyboardInterrupt use --full-trace)\u001b[0m\n",
            "\u001b[31m======================= \u001b[31m\u001b[1m229 failed\u001b[0m\u001b[31m in 120.01s (0:02:00)\u001b[0m\u001b[31m ========================\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ninja --version\n",
        "! echo $?"
      ],
      "metadata": {
        "id": "veYlbMWtDzDz",
        "outputId": "d51c2d3d-3955-404b-80ef-5f887cc78abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0.git.kitware.jobserver-pipe-1\n",
            "0\n"
          ]
        }
      ]
    }
  ]
}